{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import time\n","import os\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib as mpl\n","import torch\n","import torch.utils.data\n","\n","from torch import nn, optim\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","from torchsummary import summary\n","from IPython import display"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Using device: cpu\n"}],"source":["SAVE_PATH = 'checkpoints'\n","if not os.path.exists(SAVE_PATH):\n","    os.mkdir(SAVE_PATH)\n","PLOTS_PATH = 'plots'\n","if not os.path.exists(PLOTS_PATH):\n","    os.mkdir(PLOTS_PATH)\n","%matplotlib inline\n","mpl.style.use('dark_background')\n","np.random.seed(42)\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f'Using device: {device}')"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Uncertainty Quantification\n","\n","## Types of uncertainty\n","\n","**Epistemic (or systematic) uncertainty** is related to inadequate modelling of the data-generating process due to limited data and prior knowledge. It is due to properties of the data generation that one could in principle know, but does not.\n","\n","**Aleatoric (or statistical) uncertainty** is due to stochasticity inherent to the data-generating process itself. This differs from systematic uncertainty in the sense that aleatoric uncertainty cannot be removed but must rather be modelled by estimation of its size.\n","\n","Besides quantifying the uncertainties, another goal of uncertainty quantification is to reduce epistemic uncertainties to aleatoric ones.\n","\n","## Uncertainty and neural networks\n","Neural networks are well-known to be powerful function approximators and they posess good interpolation abilities. Their ability to extrapolate (or generalize) to the outside of the training data domain however is often poor while the probabilities they assign their predictions in these cases often stay very high."]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Data generating process with aleatory uncertainty\n","Let $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ be the data-generating process for the example considered in this notebook. The function will include some stochasticity, simulated here by the normal random variable $\\epsilon$. We can write the mapping of any input $x_i$ to the output $y_i$ as\n","\n","$$ y_i = f(x_i) + \\epsilon_i, \\hspace{1cm} \\epsilon_i \\sim \\mathcal{N}(0,\\sigma^2)$$\n","\n","Concretely, Let\n","\n","$$f(x_i) = 2 \\sin(x_i + 0.5) + 3 \\cos(0.7 x_i)$$\n","\n","Let's sample some training, validation and test data from this function and visualize it."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Data generating function and data\n","N_SAMPLES = 300  # Training data samples\n","SIGMA = 1  # Data standard deviation\n","\n","PLOT_EVERY = 10\n","BATCH_SIZE = 16\n","\n","N_EPOCHS = 750\n","DIMS = [30, 30, 20]\n","N_ENSEMBLE_MODELS = 5"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Data generating function\n","def f(x, sigma=0):\n","    y = 2 * np.sin(x + 0.5) + 3 * np.cos(0.7 * x)\n","    eps = np.random.normal(loc=0, scale=sigma, size=x.shape)\n","    return y + eps"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Training data\n","x_1 = np.random.uniform(-13, -2, size=N_SAMPLES // 2)\n","x_2 = np.random.uniform(0, 13, size=N_SAMPLES // 2)\n","x_train = np.concatenate([x_1, x_2]).reshape(N_SAMPLES, 1)\n","y_train = f(x_train, sigma=SIGMA)\n","\n","# Validation data\n","x_1 = np.random.uniform(-13, -2, size=N_SAMPLES // (2 * 5))\n","x_2 = np.random.uniform(0, 13, size=N_SAMPLES // (2 * 5))\n","x_val = np.concatenate([x_1, x_2]).reshape(N_SAMPLES // 5, 1)\n","y_val = f(x_val, sigma=SIGMA)\n","\n","# Test data (\n","x_test = np.random.uniform(-20, 20, size=N_SAMPLES // 5).reshape(N_SAMPLES // 5, 1)\n","y_test = f(x_test, sigma=SIGMA)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["x_plot = np.linspace(-30, 30, num=200)\n","fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n","ax.plot(x_plot, f(x_plot), label='Data generating function', color='C0')\n","ax.fill_between(x_plot, f(x_plot) - SIGMA, f(x_plot) + SIGMA, alpha=0.3, color='C0', label='Aleatoric uncertainty (1σ)')\n","ax.fill_between(x_plot, f(x_plot) - 2 * SIGMA, f(x_plot) + 2 * SIGMA, alpha=0.2, color='C0', label='Aleatoric uncertainty (1σ)')\n","ax.scatter(x_train, y_train, color='C1', label='Training data')\n","ax.scatter(x_val, y_val, color='C2', label='Validation data')\n","ax.scatter(x_test, y_test, color='C3', label='Testing data')\n","ax.legend()\n","fig.savefig(os.path.join(PLOTS_PATH, 'data-generating-function.pdf'))"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Datasets and training loops\n","The below defines datasets on the sampled data and the training and test loop code."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Datset and loader\n","dataset_train = torch.utils.data.TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n","dataset_val = torch.utils.data.TensorDataset(torch.Tensor(x_val), torch.Tensor(y_val))\n","dataset_test = torch.utils.data.TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n","\n","dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=BATCH_SIZE)\n","dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=BATCH_SIZE)\n","dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def train_epoch(epoch, model, loss, optimizer, dataloader):\n","    model.train()\n","    total_loss = 0\n","    for i, (x_batch, y_batch) in enumerate(dataloader, start=1):\n","        x_batch.to(device)\n","        y_batch.to(device)\n","        optimizer.zero_grad()\n","        output = model.forward(x_batch)\n","        loss_batch = loss(y_batch, output)\n","        loss_batch.backward()\n","        optimizer.step()\n","        total_loss += loss_batch.item()\n","\n","    return total_loss / i\n","\n","\n","def test_epoch(epoch, model, loss, dataloader, eval_mode=True):\n","    if eval_mode:\n","        model.eval()\n","    else:\n","        model.train()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for i, (x_batch, y_batch) in enumerate(dataloader, start=1):\n","            x_batch.to(device)\n","            y_batch.to(device)\n","            output = model.forward(x_batch)\n","            loss_batch = loss(y_batch, output)\n","            total_loss += loss_batch.item()\n","\n","    return total_loss / i\n","\n","\n","def train(model, optimizer, loss, dataloader_train, dataloader_val, dataloader_test, n_epochs, fig=None, ax_train=None, ax_func=None, do_plot=True):\n","    if do_plot and fig is None and ax_train is None and ax_func is None:\n","        fig, (ax_train, ax_func) = plt.subplots(2, 1, figsize=(16, 15))\n","\n","    train_losses, val_losses, test_losses = [], [], []\n","    t_plotted = time.time()\n","    for epoch in range(n_epochs):\n","        train_loss = train_epoch(epoch, model, loss, optimizer, dataloader_train)\n","        val_loss = test_epoch(epoch, model, loss, dataloader_val)\n","        test_loss = test_epoch(epoch, model, loss, dataloader_test)\n","\n","        train_losses.append(train_loss)\n","        val_losses.append(val_loss)\n","        test_losses.append(test_loss)\n","\n","        if do_plot and (epoch == 0 or epoch == N_EPOCHS - 1 or PLOT_EVERY < time.time() - t_plotted):\n","            t_plotted = time.time()\n","\n","            plot_learning_curve(ax_train, epoch, {'Train': train_losses, 'Validation': val_losses, 'Test': test_losses})\n","            plot_learned_function(ax_func, model, x_plot)\n","\n","            display.display(fig)\n","            display.clear_output(wait=True)\n","\n","        print(f'Epoch {epoch:3d} | Train loss {train_loss:2.3f} | Val loss {val_loss:2.3f} | Test loss {test_loss:2.3f}', end='\\r')\n","\n","\n","def plot_learned_function(ax, model, x_plot):\n","    with torch.no_grad():\n","        x_plot_tensor = torch.Tensor(x_plot.reshape(-1, 1))\n","        x_plot_tensor.to(device)\n","        y_pred, pred_variance = model.predict(x_plot_tensor)\n","        y_pred = y_pred.flatten()\n","        if isinstance(pred_variance, tuple):\n","            aleatory_std = pred_variance[0].flatten().sqrt()\n","            epistemic_std = pred_variance[1].flatten().sqrt()\n","        else:\n","            aleatory_std = pred_variance.flatten().sqrt()\n","            epistemic_std = None\n","\n","    ax.clear()\n","    ax.plot(x_plot, f(x_plot), color='C0', label='Data generating function')\n","    ax.scatter(x_train, y_train, color='C0', label='Training data')\n","    ax.scatter(x_val, y_val, color='C2', label='Validation data')\n","    ax.scatter(x_test, y_test, color='C3', label='Testing data')\n","    ax.plot(x_plot, y_pred, color='C1', label='Learned mean function')\n","    for i in range(3):\n","        ax.fill_between(\n","            x_plot,\n","            y_pred - (i + 1) * aleatory_std,\n","            y_pred + (i + 1) * aleatory_std,\n","            alpha=0.3 - 0.05 * i,\n","            color='C1',\n","            label=f'Aleatory uncertainty ({i + 1}σ)'\n","        )\n","    if epistemic_std is not None:\n","        for i in range(3):\n","            # Above\n","            ax.fill_between(\n","                x_plot,\n","                y_pred + 3 * aleatory_std,\n","                y_pred + 3 * aleatory_std + (i + 1) * epistemic_std,\n","                alpha=0.3 - 0.05 * i,\n","                color='C3',\n","                label=f'Epistemic uncertainty ({i + 1}σ)'\n","            )\n","            # Below\n","            ax.fill_between(\n","                x_plot,\n","                y_pred - 3 * aleatory_std,\n","                y_pred - 3 * aleatory_std - (i + 1) * epistemic_std,\n","                alpha=0.3 - 0.05 * i,\n","                color='C3',\n","            )\n","    ax.set_ylim([-10, 10])\n","    ax.legend()\n","\n","\n","def plot_learning_curve(ax, epoch, loss_series):\n","    ax.clear()\n","    ax.set_ylabel('Loss')\n","    ax.set_xlabel('Epoch')\n","    ax.set_title('Learning curves')\n","    ax.set_yscale('log')\n","    for label, series in loss_series.items():\n","        ax.plot(range(epoch + 1), series, label=label)\n","    ax.legend()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# Methods for modelling uncertainty\n","\n","Below we will go through different methods for modelling uncertainty with Neural Networks.\n","\n","1. Monte Carlo Dropout\n","2. Distributional Parameter Learning\n","3. Mixture of Distributional Parameter Learning Models (Ensembling of several models)\n","4. Mixture of Distributional Parameter Learning Models (Monte Carlo Dropout simulated ensemble on single model)\n","5. Bayesian Neural Networks\n","\n","As with the task, the model we will use will be shared among the methods."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class BaseModel(nn.Module):\n","    \"\"\"Base model shared for all methods\"\"\"\n","\n","    def __init__(self, dims, dropout_rates, batchnorm):\n","        super().__init__()\n","        self.layers = []\n","\n","        if dropout_rates:\n","            self.layers.append(nn.Dropout(p=dropout_rates[0]))\n","        self.layers.append(nn.Linear(dims[0], dims[1]))\n","        for i in range(1, len(dims) - 1):\n","            if batchnorm:\n","                self.layers.append(nn.BatchNorm1d(dims[i]))\n","            self.layers.append(nn.ReLU())\n","            if dropout_rates:\n","                self.layers.append(nn.Dropout(p=dropout_rates[i]))\n","            self.layers.append(nn.Linear(dims[i], dims[i + 1]))\n","\n","        for i, layer in enumerate(self.layers):\n","            setattr(self, layer.__class__.__name__ + f'_{i}', layer)\n","\n","    def forward(self, x):\n","        \"\"\"Method computing a single forward pass of the model return the values needed to evaluate the loss function.\"\"\"\n","        raise NotImplementedError()\n","\n","    def predict(self, x):\n","        \"\"\"Method expected to return a dictionary of mean prediction along with estimated aleatory and epistemic uncertainty.\"\"\"\n","        raise NotImplementedError()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Monte Carlo Dropout\n","As proposed in \\[1\\], the widely used regularization technique *Dropout* can be used to provide uncertainty estimates for a models predictions. This is in line with the interpretation of dropout as a form of model ensembling or averaging \\[2\\].\n","\n","Typically, dropout is applied at train-time and deactivated at test-time in order to exploit the full power of the \"model ensemble\". If instead we also apply dropout at test-time and do several forward passes of the same input $x$, we can expect to get a different prediction every time with some variance. We can use this to deduce an empirical distribution over the outputs given the inputs $p(y|x)$. In turn. we can then use this distribution to estimate an empirical mean and variance of the mean of the predictions.\n","\n","We expect that the empirical variance of the mean is low for inputs originating from dense regions of the input space since more sub-networks were presented with data from these regions. In more sparsely sampled regions of the input space or regions with no data at all, we expect the spread in sub-network behaviour to be higher. This corresponds to the desired behavior for the uncertainty estimation."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class MCDModel(BaseModel):\n","    def __init__(self, dims=[1, *DIMS, 1], dropout_rates=[0.0, 0.2, 0.2, 0.1], batchnorm=False):\n","        \"\"\"\n","        Args:\n","            dims (list): Number of units in each of the layers including input and output.\n","            rates (list): Probability of dropping any activation in the representation immediately before any linear transformation.\n","        \"\"\"\n","        super().__init__(dims=dims, dropout_rates=dropout_rates, batchnorm=batchnorm)\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        return x\n","\n","    def predict(self, x, n_repeats=40):\n","        self.train()\n","        y_pred = []\n","        for i in range(n_repeats):\n","            y_pred.append(self.forward(x))\n","        y_pred = torch.stack(y_pred)  # Create new axis (Repeats, Batch, Dims)\n","        return torch.mean(y_pred, axis=0), torch.var(y_pred, axis=0)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Initialize model parameters and setup\n","mcd_model = MCDModel()\n","mcd_model.to(device)\n","print(summary(model=mcd_model, input_size=(1,), device=device))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["mse_loss = nn.MSELoss(reduction='mean')\n","mse_loss.to(device)\n","optimizer = optim.Adam(mcd_model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fig, (ax_train, ax_func) = plt.subplots(2, 1, figsize=(16, 15))\n","train(mcd_model, optimizer, mse_loss, dataloader_train, dataloader_val, dataloader_test, 1000, fig=fig, ax_train=ax_train, ax_func=ax_func)\n","torch.save(mcd_model.state_dict(), os.path.join(SAVE_PATH, f'{mcd_model.__class__.__name__}_model_state_dict.pkl'))\n","fig.savefig(os.path.join(PLOTS_PATH, f'{mcd_model.__class__.__name__}-evaluation.pdf'))"]},{"cell_type":"markdown","execution_count":227,"metadata":{},"outputs":[],"source":["## Distributional Parameter Learning"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class GaussianNLLLoss(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, mu_true, pred_parameters):\n","        mu_pred, logvar_pred = pred_parameters\n","        return torch.mean(logvar_pred / (2) + (mu_true - mu_pred).pow(2) / (2 * logvar_pred.exp()), axis=0)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class DPLModel(BaseModel):\n","    \"\"\"Distributional Parameter Learning Model using Gaussian\"\"\"\n","    def __init__(self, dims=[1, *DIMS, 2], dropout_rates=[], batchnorm=False):\n","        super().__init__(dims=dims, dropout_rates=dropout_rates, batchnorm=batchnorm)\n","\n","    def forward(self, x):\n","        for layer in self.layers:\n","            x = layer(x)\n","        mu = x[..., 0:1]\n","        logvar = x[..., 1:2]\n","        return mu, logvar\n","\n","    def predict(self, x):\n","        mu, logvar = self.forward(x)\n","        return mu, logvar.exp()"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Initialize model parameters and setup\n","dpl_model = DPLModel()\n","dpl_model.to(device)\n","print(summary(model=dpl_model, input_size=(1,), device=device))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["loss = GaussianNLLLoss()\n","loss.to(device)\n","optimizer = optim.Adam(dpl_model.parameters(), lr=3e-4)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fig, (ax_train, ax_func) = plt.subplots(2, 1, figsize=(16, 15))\n","train(dpl_model, optimizer, loss, dataloader_train, dataloader_val, dataloader_test, 1000, fig=fig, ax_train=ax_train, ax_func=ax_func)\n","torch.save(dpl_model.state_dict(), os.path.join(SAVE_PATH, f'{dpl_model.__class__.__name__}_model_state_dict.pkl'))\n","fig.savefig(os.path.join(PLOTS_PATH, f'{dpl_model.__class__.__name__}-evaluation.pdf'))"]},{"cell_type":"markdown","execution_count":41,"metadata":{},"outputs":[{"ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-41-b01131210951>, line 3)","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-b01131210951>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    Combine several distributional parameter learning models each of which having learned a Gaussian distribution over the output given the input into a Guasisna mixture distribution.\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["## Distributional Parameter Learning Ensembles\n","\n","Combine several distributional parameter learning models each of which having learned a Gaussian distribution over the output given the input into a Guasisna mixture distribution.\n","\n","Compute the mean and variance of the mixture distribution and use that as the ensemble prediction.\n","\n","\n","### Ensembling by training several networks\n","\n","The most obvious way to obtain an ensemble of models is by simply training several models and combining their predictions into a single prediction."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class EnsembleDPLModel(nn.Module):\n","    \"\"\"Ensembling model\"\"\"\n","    def __init__(self, models):\n","        super().__init__()\n","        self.models = models\n","        for i_model, model in enumerate(self.models):\n","            setattr(self, f'ensemble_model_{i_model}', model)\n","\n","    def forward(self, x):\n","        mus, logvars = [], []\n","        for model in self.models:\n","            mu, logvar = model.forward(x)\n","            mus.append(mu)\n","            logvars.append(logvar)\n","        return torch.stack(mus), torch.stack(logvars)\n","\n","    def predict(self, x):\n","        mus, logvars = self.forward(x)\n","        mean_of_mu = torch.mean(mus, axis=0)\n","        mean_of_variance  = torch.mean(logvars.exp(), axis=0)\n","        variance_of_mu =  torch.mean(mus.pow(2) - mean_of_mu.pow(2), axis=0)\n","\n","        return mean_of_mu, (mean_of_variance, variance_of_mu)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(N_ENSEMBLE_MODELS, 2, figsize=(16, 20))\n","ensemble_models = []\n","for i in range(N_ENSEMBLE_MODELS):\n","    dpl_model = DPLModel()\n","    dpl_model.to(device)\n","    ensemble_models.append(dpl_model)\n","\n","for i, model in enumerate(ensemble_models):\n","    print(f'Model {i}')\n","    gaussian_nll_loss = GaussianNLLLoss()\n","    gaussian_nll_loss.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","    train(model, optimizer, gaussian_nll_loss, dataloader_train, dataloader_val, dataloader_test, 1000, fig=fig, ax_train=axes[i, 0], ax_func=axes[i, 1])\n","    torch.save(model.state_dict(), os.path.join(SAVE_PATH, f'{model.__class__.__name__}_ensemble_{i}_model_state_dict.pkl'))\n","\n","fig.savefig(os.path.join(PLOTS_PATH, f'{EnsembleDPLModel.__class__.__name__}-evaluation.pdf'))    "]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Collect the mod\n","ensemble_dpl_model = EnsembleDPLModel(ensemble_models)\n","ensemble_dpl_model = ensemble_dpl_model.to(device)\n","torch.save(ensemble_dpl_model.state_dict(), os.path.join(SAVE_PATH, f'{ensemble_dpl_model.__class__.__name__}_model_state_dict.pkl'))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n","plot_learned_function(ax, ensemble_dpl_model, x_plot)\n","display.display(fig)\n","display.clear_output(wait=True)\n","fig.savefig(os.path.join(PLOTS_PATH, f'{ensemble_dpl_model.__class__.__name__}-evaluation-func.pdf'))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["with torch.no_grad():\n","    x_plot_tensor = torch.Tensor(x_plot.reshape(-1, 1))\n","    x_plot_tensor.to(device)\n","    y_pred, (aleatory_var, epistemic_var) = ensemble_dpl_model.predict(x_plot_tensor)\n","    y_pred = y_pred.flatten()\n","    aleatory_std = aleatory_var.flatten().sqrt()\n","    epistemic_std = epistemic_var.flatten().sqrt()\n","\n","fig, ax = plt.subplots(1, 1, figsize=(16, 9))\n","ax.plot(x_plot, aleatory_std, label='Aleatory uncertainty')\n","ax.plot(x_plot, epistemic_std, label='Epistemic uncertainty')\n","ax.legend()"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["### Ensembling by Monte Carlo Dropout\n","We can also simulate an ensemble in a single model by training it with dropout and keeping dropout on for inference."]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["class MCDDPLModel(DPLModel):\n","    \"\"\"Distributional Parameter Learning Model using Gaussian\"\"\"\n","    def __init__(self, dims=[1, *DIMS, 2], dropout_rates=[0.0, 0.2, 0.2, 0.1], batchnorm=False):\n","        super().__init__(dims=dims, dropout_rates=dropout_rates, batchnorm=batchnorm)\n","\n","    def predict(self, x, n_repeats=100):\n","        self.train()\n","        mus, logvars = [], []\n","        for i in range(n_repeats):\n","            mu, logvar = self.forward(x)\n","            mus.append(mu)\n","            logvars.append(logvar)\n","\n","        mus, logvars = torch.stack(mus), torch.stack(logvars)  # Create new axis (Repeats, Batch, Dims)\n","        mean_of_mu = torch.mean(mus, axis=0)\n","        mean_of_variance  = torch.mean(logvars.exp(), axis=0)\n","        variance_of_mu =  torch.mean(mus.pow(2) - mean_of_mu.pow(2), axis=0)\n","        return mean_of_mu, (mean_of_variance, variance_of_mu)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["# Initialize model parameters and setup\n","mcddpl_model = MCDDPLModel()\n","mcddpl_model.to(device)\n","print(summary(model=mcddpl_model, input_size=(1,), device=device))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["gaussian_nll_loss = GaussianNLLLoss()\n","gaussian_nll_loss.to(device)\n","optimizer = optim.Adam(mcddpl_model.parameters(), lr=1e-3)"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["fig, (ax_train, ax_func) = plt.subplots(2, 1, figsize=(16, 15))\n","train(mcddpl_model, optimizer, loss, dataloader_train, dataloader_val, dataloader_test, 1000, fig=fig, ax_train=ax_train, ax_func=ax_func)\n","torch.save(mcddpl_model.state_dict(), os.path.join(SAVE_PATH, f'{mcddpl_model.__class__.__name__}_model_state_dict.pkl'))\n","fig.savefig(os.path.join(PLOTS_PATH, f'{mcddpl_model.__class__.__name__}-evaluation.pdf'))"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## Kullback-Leibler divergence of predicted and true data distribution"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["def kl_divergence(mu_true, var_true, mu_pred, var_pred):\n","    return torch.log(var_pred / var_true) + (var_true.pow(2) + (mu_true - mu_pred).pow(2)) / (2 * var_pred.pow(2)) - 0.5"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["n_samples_list = list(range(0, 1000, 100))"]},{"cell_type":"code","execution_count":0,"metadata":{},"outputs":[],"source":["for n_samples in n_samples_list:\n","    x_1 = np.random.uniform(-13, -2, size=n_samples // 2)\n","    x_2 = np.random.uniform(0, 13, size=n_samples // 2)\n","    x_train = np.concatenate([x_1, x_2]).reshape(n_samples, 1)\n","    y_train = f(x_train, sigma=SIGMA)\n","\n","    # MCD Model\n","    mcd_model = MCDModel()\n","    mcd_model.to(device)\n","    mse_loss = nn.MSELoss(reduction='mean')\n","    mse_loss.to(device)\n","    optimizer = optim.Adam(mcd_model.parameters(), lr=1e-3)\n","    train(mcddpl_model, optimizer, gaussian_nll_loss, dataloader_train, dataloader_val, dataloader_test, 1000)\n","\n","    # DPL Model\n","    dpl_model = DPLModel()\n","    dpl_model.to(device)\n","    gaussian_nll_loss = GaussianNLLLoss()\n","    gaussian_nll_loss.to(device)\n","    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n","    train(mcddpl_model, optimizer, gaussian_nll_loss, dataloader_train, dataloader_val, dataloader_test, 1000)\n","\n","    # MCDDPL Model\n","    mcddpl_model = MCDDPLModel()\n","    mcddpl_model.to(device)\n","    gaussian_nll_loss = GaussianNLLLoss()\n","    gaussian_nll_loss.to(device)\n","    optimizer = optim.Adam(mcddpl_model.parameters(), lr=1e-3)\n","    train(mcddpl_model, optimizer, gaussian_nll_loss, dataloader_train, dataloader_val, dataloader_test, 1000)\n","    "]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["## To do\n","- Quantile Regression\n","- Gaussian process baseline\n","- Write methodology of distributional parameter learning and Gaussian mixture of such models (ensemble)\n","- Kullback-Leibler divergence between true data generating function/distribution and the modelled one for each of the methods in a plot as function of training time\n","\n"]},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":["# References\n","[1] Yarin Gal. 2017. Uncertainty in Deep Learning. PhD Thesis.\n","\n","[2] Nitish, Hinton, et al. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overﬁtting.\n","\n","\n","[10] https://en.wikipedia.org/wiki/Uncertainty_quantification\n","\n","[11] https://www.inovex.de/blog/uncertainty-quantification-deep-learning/"]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}